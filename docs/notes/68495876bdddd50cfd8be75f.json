{
  "online_content": {
    "type": "doc",
    "content": [
      {
        "type": "heading",
        "attrs": {
          "level": 1
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Automating Inequality"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "When Obermeyer and his colleague conducted routine statistical checks on data they received from a large hospital, they were surprised to find that people who self-identified as Black were generally assigned lower risk scores than equally sick White people. Black individuals were less likely to be referred to programs that provide more personalized care."
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "The data revealed that the average Black person was also substantially sicker than the average White person and that the care provided to Black people cost an average of US$1,800 less per year than the care given to a White person with the same number of chronic health problems."
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "The algorithm determined who should receive more intensive care based on healthcare costs, inadvertently perpetuating these biases. As a result, Black patients needed to be significantly sicker than their White counterparts to qualify for additional care. Under this algorithm, only 17.7% of patients selected for extra care were Black. However, the researchers estimated that this number would rise to 46.5% if the algorithm operated without racial bias."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Opaque Algorithms"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "The evident racial bias in the decision-making software utilized by U.S. hospitals highlights the reality of our world: one increasingly governed by automated decisions made by opaque algorithms. These algorithms, often understood only by a select few, have profound impacts on our lives. We generally assume that developers aim to create systems that do not harm society. However, the complexity inherent in data processing can lead to misunderstandings or the inadvertent introduction of biases in model creation, especially when that bias comes in no straightforward variable that indicates the bias. In this case the healthcare cost."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Scale and Damage"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "Cathy O’Neil, in her influential book ‘Weapons of Math Destruction,’ brings to light the enormous scale and potential damage caused by these opaque algorithms. She argues that they often function as unregulated, unexamined decision-makers in crucial areas like employment, education, and, as highlighted, healthcare. These automated systems, designed to process vast amounts of data, can make life-altering decisions without oversight or accountability. O’Neil warns that these algorithms, while intended to be objective and efficient, can instead replicate and magnify human biases, particularly when the data they process is tainted with historical and social prejudices."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Bias and Fairness"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "Moreover, O’Neil stresses the importance of addressing the inherent biases within these algorithms. She posits that the lack of transparency in algorithmic decision-making processes often cloaks the prejudice embedded in their design and data. This is particularly concerning because these biases are not always overt; they can be subtle, and hidden in the nuances of the data, like in the healthcare costs example, where monetary values can indirectly encode racial biases. O’Neil’s work calls for a critical examination of these systems, urging developers and users alike to question not just the efficiency of an algorithm but its ethical implications and fairness. Her book serves as a wake-up call to the dangers of blindly trusting algorithmic systems and emphasizes the need for more responsible and equitable data science practices.”"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "When teams develop a model, they often ask, ‘Does this algorithm work for me?’ but seldom consider, ‘Does it work for others?’ We need to redefine what it means for an algorithm to ‘work.’ In today’s world, algorithms play a crucial role in making vital decisions that affect the daily lives of people. Alarmingly, many of these algorithms are shrouded in secrecy and are often unfair, posing a threat to society."
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "Instead of merely asking, ‘Does it work?’, we should be questioning, ‘What is the evidence that it works? What proof do we have of its fairness?"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "text": "References:"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "link",
                "attrs": {
                  "href": "https://www.nature.com/articles/d41586-019-03228-6",
                  "target": "_blank",
                  "rel": "noopener noreferrer nofollow",
                  "class": "text-muted-foreground underline underline-offset-[3px] hover:text-primary transition-colors cursor-pointer"
                }
              },
              {
                "type": "underline"
              }
            ],
            "text": "https://www.nature.com/articles/d41586-019-03228-6"
          }
        ]
      }
    ]
  },
  "online_content_time": "2025-06-11T10:24:09.011Z"
}