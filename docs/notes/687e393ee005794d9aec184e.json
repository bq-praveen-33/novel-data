{
  "online_content": {
    "type": "doc",
    "content": [
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Last Updated : 11 Jul, 2025"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Q-Learning is a popular "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "link",
                "attrs": {
                  "href": "https://www.geeksforgeeks.org/machine-learning/model-free-reinforcement-learning-an-overview/",
                  "target": "_blank",
                  "rel": "noopener",
                  "class": "text-muted-foreground underline underline-offset-[3px] hover:text-primary transition-colors cursor-pointer"
                }
              },
              {
                "type": "bold"
              },
              {
                "type": "underline"
              }
            ],
            "text": "model-free reinforcement learning algorithm"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": " that helps an agent learn how to make the best decisions by interacting with its environment. Instead of needing a model of the environment the agent learns purely from experience by trying different actions and seeing their results"
          }
        ]
      },
      {
        "type": "blockquote",
        "content": [
          {
            "type": "paragraph",
            "content": [
              {
                "type": "text",
                "marks": [
                  {
                    "type": "textStyle",
                    "attrs": {
                      "color": ""
                    }
                  },
                  {
                    "type": "italic"
                  }
                ],
                "text": "Imagine a system that sees an apple but incorrectly says,"
              },
              {
                "type": "text",
                "marks": [
                  {
                    "type": "italic"
                  }
                ],
                "text": " “It’s a mango.” The system is told, “Wrong! It’s an apple.” It learns from this mistake. Next time, when shown the apple, it correctly says “It’s an apple.” This trial-and-error process, guided by feedback is like how Q-Learning works."
              }
            ]
          }
        ]
      },
      {
        "type": "image",
        "attrs": {
          "src": "https://media.geeksforgeeks.org/wp-content/uploads/20250529093140146677/Q-Learning-660.png",
          "alt": "Q-Learning-660",
          "title": null,
          "width": 660,
          "height": 330
        }
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "italic"
              }
            ],
            "text": "Q Learning"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The core idea is that the agent builds a Q-table which stores Q-values. Each Q-value estimates how good it is to take a specific action in a given state in terms of the expected future rewards. Over time the agent updates this table using the feedback it receives"
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Key Components of Q-learning"
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "1. Q-Values or Action-Values"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Q-values represent the expected rewards for taking an action in a specific state. These values are updated over time using the Temporal Difference (TD) update rule."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "2. Rewards and Episodes"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The agent moves through different states by taking actions and receiving rewards. The process continues until the agent reaches a terminal state which ends the episode."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "3. Temporal Difference or TD-Update"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The agent updates Q-values using the formula:"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Q(S,A)←Q(S,A)+α(R+γQ(S′,A′)−Q(S,A))"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "Q"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "S"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ","
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "A"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ")←"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "Q"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "S"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ","
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "A"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ")+"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "α"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "R"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "+"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "γQ"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "S"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "′,"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "A"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "′)−"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "Q"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "S"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ","
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "A"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "))"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Where,"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "S"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the current state."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "A"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the action taken by the agent."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "S'"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the next state the agent moves to."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "A'"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the best next action in state S'."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "R"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the reward received for taking action A in state S."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "γ (Gamma) "
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "is the discount factor which balances immediate rewards with future rewards."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "α (Alpha)"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the learning rate determining how much new information affects the old Q-values."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "4. ϵ-greedy Policy (Exploration vs. Exploitation)"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The ϵ-greedy policy helps the agent decide which action to take based on the current Q-value estimates:"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Exploitation:"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " The agent picks the action with the highest Q-value with probability 1−ϵ1−"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      },
                      {
                        "type": "italic"
                      }
                    ],
                    "text": "ϵ"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ". This means the agent uses its current knowledge to maximize rewards."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Exploration:"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " With probability ϵ"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      },
                      {
                        "type": "italic"
                      }
                    ],
                    "text": "ϵ"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ", the agent picks a random action, exploring new possibilities to learn if there are better ways to get rewards. This allows the agent to discover new strategies and improve its decision-making over time."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "How does Q-Learning Works?"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Q-learning models follow an iterative process where different components work together to train the agent. Here's how it works step-by-step:"
          }
        ]
      },
      {
        "type": "image",
        "attrs": {
          "src": "https://media.geeksforgeeks.org/wp-content/uploads/20250529124116374250/Q-learning-algorithm.webp",
          "alt": "Q-learning-algorithm",
          "title": null,
          "width": 800,
          "height": 400
        }
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "italic"
              }
            ],
            "text": "Q learning algorithm"
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "1. "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Start at a State (S)"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The environment provides the agent with a starting state which describes the current situation or condition."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "2. "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Agent Selects an Action (A)"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Based on the current state and the agent chooses an action using its policy. This decision is guided by a Q-table which estimates the potential rewards for different state-action pairs. The agent typically uses an ε-greedy strategy:"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "It sometimes explores new actions (random choice)."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "It mostly exploits known good actions (based on current Q-values)."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "3. "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Action is Executed and Environment Responds"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The agent performs the selected action. The environment then provides:"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "A "
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "new state (S′)"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " — the result of the action."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "A "
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "reward (R)"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " — feedback on the action's effectiveness."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "4. "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Learning Algorithm Updates the Q-Table"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The agent updates the Q-table using the new experience:"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "It adjusts the value for the state-action pair based on the received reward and the new state."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "This helps the agent better estimate which actions are more beneficial over time."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "5. "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Policy is Refined and the Cycle Repeats"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "With updated Q-values the agent:"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "Improves its policy to make better future decisions."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "Continues this loop — observing states, taking actions, receiving rewards and updating Q-values across many episodes."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Over time the agent learns the optimal policy that consistently yields the highest possible reward in the environment."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Methods for Determining Q-values"
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "1. "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Temporal Difference (TD):"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Temporal Difference is calculated by comparing the current state and action values with the previous ones. It provides a way to learn directly from experience, without needing a model of the environment."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "2. "
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Bellman’s Equation:"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "link",
                "attrs": {
                  "href": "https://www.geeksforgeeks.org/machine-learning/bellman-equation/",
                  "target": "_blank",
                  "rel": "noopener",
                  "class": "text-muted-foreground underline underline-offset-[3px] hover:text-primary transition-colors cursor-pointer"
                }
              },
              {
                "type": "bold"
              },
              {
                "type": "underline"
              }
            ],
            "text": "Bellman’s Equation"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": " is a recursive formula used to calculate the value of a given state and determine the optimal action. It is fundamental in the context of Q-learning and is expressed as:"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Q(s,a)=R(s,a)+γmax⁡aQ(s′,a)"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "Q"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "s"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ","
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "a"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ")="
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "R"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "s"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ","
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "a"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ")+"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "γ"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "max"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "a"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "​"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "Q"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "("
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "s"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "′,"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              },
              {
                "type": "italic"
              }
            ],
            "text": "a"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ")"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Where:"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Q(s, a)"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the Q-value for a given state-action pair."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "R(s, a)"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the immediate reward for taking action "
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "a"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " in state "
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "s"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "γ"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " is the discount factor, representing the importance of future rewards."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "maxaQ(s′,a)"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      },
                      {
                        "type": "italic"
                      }
                    ],
                    "text": "maxa"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "​"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      },
                      {
                        "type": "italic"
                      }
                    ],
                    "text": "Q"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "("
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      },
                      {
                        "type": "italic"
                      }
                    ],
                    "text": "s"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "′,"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      },
                      {
                        "type": "italic"
                      }
                    ],
                    "text": "a"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ") is the maximum Q-value for the next state "
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "s'"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": " and all possible actions."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "What is a Q-table?"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The Q-table is essentially a memory structure where the agent stores information about which actions yield the best rewards in each state. It is a table of Q-values representing the agent's understanding of the environment. As the agent explores and learns from its interactions with the environment, it updates the Q-table. The Q-table helps the agent make informed decisions by showing which actions are likely to lead to better rewards."
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Structure of a Q-table"
          },
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": ":"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "Rows represent the states."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "Columns represent the possible actions."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": "Each entry in the table corresponds to the Q-value for a state-action pair."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Over time, as the agent learns and refines its Q-values through exploration and exploitation, the Q-table evolves to reflect the best actions for each state, leading to optimal decision-making."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Implementation of Q-Learning"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Here, we implement basic Q-learning algorithm where agent learns the optimal action-selection strategy to reach a goal state in a grid-like environment."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Step 1: Define the Environment"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Set up the environment parameters including the number of states and actions and initialize the Q-table. In this each state represents a position and actions move the agent within this environment."
          }
        ]
      },
      {
        "type": "codeBlock",
        "attrs": {
          "language": null
        },
        "content": [
          {
            "type": "text",
            "text": "import numpy as np\n\nn_states = 16  \nn_actions = 4  \ngoal_state = 15  \n\nQ_table = np.zeros((n_states, n_actions))\n"
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Step 2: Set Hyperparameters"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Define the parameters for the Q-learning algorithm which include the learning rate, discount factor, exploration probability and the number of training epochs."
          }
        ]
      },
      {
        "type": "codeBlock",
        "attrs": {
          "language": null
        },
        "content": [
          {
            "type": "text",
            "text": "learning_rate = 0.8\ndiscount_factor = 0.95\nexploration_prob = 0.2\nepochs = 1000\n"
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Step 3: Implement the Q-Learning Algorithm"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Perform the Q-learning algorithm over multiple epochs. Each epoch involves selecting actions based on an epsilon-greedy strategy updating Q-values based on rewards received and transitioning to the next state."
          }
        ]
      },
      {
        "type": "codeBlock",
        "attrs": {
          "language": null
        },
        "content": [
          {
            "type": "text",
            "text": "for epoch in range(epochs):\n    current_state = np.random.randint(0, n_states)  \n    while current_state != goal_state:\n        if np.random.rand() < exploration_prob:\n            action = np.random.randint(0, n_actions)  \n        else:\n            action = np.argmax(Q_table[current_state])  \n\n        next_state = (current_state + 1) % n_states\n\n        reward = 1 if next_state == goal_state else 0\n\n        Q_table[current_state, action] += learning_rate * \\\n            (reward + discount_factor *\n             np.max(Q_table[next_state]) - Q_table[current_state, action])\n\n        current_state = next_state  \n"
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 3
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Step 4: Output the Learned Q-Table"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "After training, print the Q-table to examine the learned Q-values which represent the expected rewards for taking specific actions in each state."
          }
        ]
      },
      {
        "type": "codeBlock",
        "attrs": {
          "language": null
        },
        "content": [
          {
            "type": "text",
            "text": "q_values_grid = np.max(Q_table, axis=1).reshape((4, 4)) \n\n# Plot the grid of Q-values\nplt.figure(figsize=(6, 6))\nplt.imshow(q_values_grid, cmap='coolwarm', interpolation='nearest')\nplt.colorbar(label='Q-value')\nplt.title('Learned Q-values for each state')\nplt.xticks(np.arange(4), ['0', '1', '2', '3'])\nplt.yticks(np.arange(4), ['0', '1', '2', '3'])\nplt.gca().invert_yaxis()  # To match grid layout\nplt.grid(True)\n\n# Annotating the Q-values on the grid\nfor i in range(4):\n    for j in range(4):\n        plt.text(j, i, f'{q_values_grid[i, j]:.2f}', ha='center', va='center', color='black')\n\nplt.show()\n\n# Print learned Q-table\nprint(\"Learned Q-table:\")\nprint(Q_table)\n"
          }
        ]
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "bold"
              }
            ],
            "text": "Output:"
          }
        ]
      },
      {
        "type": "image",
        "attrs": {
          "src": "https://media.geeksforgeeks.org/wp-content/uploads/20250529095725581244/Q-Table-.png",
          "alt": "Q-Table-",
          "title": null,
          "width": 508,
          "height": 496
        }
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "italic"
              }
            ],
            "text": "Q values on grid"
          }
        ]
      },
      {
        "type": "image",
        "attrs": {
          "src": "https://media.geeksforgeeks.org/wp-content/uploads/20250225203315934122/Capture.PNG",
          "alt": "Capture",
          "title": null,
          "width": 572,
          "height": 451
        }
      },
      {
        "type": "paragraph",
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "The learned Q-table shows the expected rewards for each state-action pair, with higher Q-values near the goal state (state 15), indicating the optimal actions that lead to reaching the goal. The agent's actions gradually improve over time, as reflected in the increasing Q-values across states leading to the goal."
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Advantages of Q-learning"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Trial and Error Learning"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": Q-learning improves over time by trying different actions and learning from experience."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Self-Improvement"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": Mistakes lead to learning, helping the agent avoid repeating them."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Better Decision-Making"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": Stores successful actions to avoid bad choices in future situations."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Autonomous Learning"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": It learns without external supervision, purely through exploration."
                  }
                ]
              }
            ]
          }
        ]
      },
      {
        "type": "heading",
        "attrs": {
          "level": 2
        },
        "content": [
          {
            "type": "text",
            "marks": [
              {
                "type": "textStyle",
                "attrs": {
                  "color": ""
                }
              }
            ],
            "text": "Disadvantages of Q-learning"
          }
        ]
      },
      {
        "type": "bulletList",
        "attrs": {
          "tight": true
        },
        "content": [
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Slow Learning"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": Requires many examples, making it time-consuming for complex problems."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Expensive in Some Environments"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": In robotics, testing actions can be costly due to physical limitations."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Curse of Dimensionality"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": Large state and action spaces make the Q-table too large to handle efficiently."
                  }
                ]
              }
            ]
          },
          {
            "type": "listItem",
            "content": [
              {
                "type": "paragraph",
                "content": [
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "bold"
                      }
                    ],
                    "text": "Limited to Discrete Actions"
                  },
                  {
                    "type": "text",
                    "marks": [
                      {
                        "type": "textStyle",
                        "attrs": {
                          "color": ""
                        }
                      }
                    ],
                    "text": ": It struggles with continuous actions like adjusting speed, making it less suitable for real-world applications involving continuous decisions."
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  },
  "online_content_time": "2025-07-23T06:50:43.212Z"
}